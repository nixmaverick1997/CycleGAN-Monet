{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom matplotlib import pyplot as plt\nimport os\nfrom torch.utils.data import Dataset\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-02-18T14:59:16.732846Z","iopub.execute_input":"2022-02-18T14:59:16.7338Z","iopub.status.idle":"2022-02-18T14:59:18.419354Z","shell.execute_reply.started":"2022-02-18T14:59:16.733685Z","shell.execute_reply":"2022-02-18T14:59:18.418602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n        self.conv=nn.Sequential(\n            nn.Conv2d(in_channels,out_channels,4,stride,1,bias=True,padding_mode=\"reflect\"),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU(0.2)\n        )\n    def forward(self,x):\n        return self.conv(x)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T14:59:28.742965Z","iopub.execute_input":"2022-02-18T14:59:28.743234Z","iopub.status.idle":"2022-02-18T14:59:28.74974Z","shell.execute_reply.started":"2022-02-18T14:59:28.743207Z","shell.execute_reply":"2022-02-18T14:59:28.74877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Discriminator\n- Upsampling from 64, 128, 256 and 512","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self,in_channels=3,features=[64,128,256,512]):\n        super().__init__()\n        self.initial=nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                features[0],\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                padding_mode=\"reflect\"\n            ),\n            nn.LeakyReLU(0.2)\n        )\n        layers=[]\n        in_channels=features[0]\n        for feature in features[1:]:\n            layers.append(Block(in_channels,feature,stride=1 if feature==features[-1] else 2))\n            in_channels=feature\n        layers.append(nn.Conv2d(in_channels,1,kernel_size=4,stride=1,padding=1,padding_mode=\"reflect\"))\n        self.model=nn.Sequential(*layers)\n    def forward(self, x):\n        x = self.initial(x)\n        return torch.sigmoid(self.model(x))\n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-18T14:59:31.863407Z","iopub.execute_input":"2022-02-18T14:59:31.863871Z","iopub.status.idle":"2022-02-18T14:59:31.872344Z","shell.execute_reply.started":"2022-02-18T14:59:31.863834Z","shell.execute_reply":"2022-02-18T14:59:31.871319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generator\n- Downsampling and Instance Normalization","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            # Initial convolution block\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(3, 64, 7),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n\n            # Downsampling\n            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n            nn.InstanceNorm2d(256),\n            nn.ReLU(inplace=True),\n\n            # Residual blocks\n            ResidualBlock(256),\n            ResidualBlock(256),\n            ResidualBlock(256),\n            ResidualBlock(256),\n            ResidualBlock(256),\n            ResidualBlock(256),\n            ResidualBlock(256),\n            ResidualBlock(256),\n            ResidualBlock(256),\n\n            # Upsampling\n            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n\n            # Output layer\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(64, 3, 7),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.main(x)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T14:59:35.471727Z","iopub.execute_input":"2022-02-18T14:59:35.471997Z","iopub.status.idle":"2022-02-18T14:59:35.482719Z","shell.execute_reply.started":"2022-02-18T14:59:35.471967Z","shell.execute_reply":"2022-02-18T14:59:35.482024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ResidualBlock, self).__init__()\n\n        self.res = nn.Sequential(nn.ReflectionPad2d(1),\n                                 nn.Conv2d(in_channels, in_channels, 3),\n                                 nn.InstanceNorm2d(in_channels),\n                                 nn.ReLU(inplace=True),\n                                 nn.ReflectionPad2d(1),\n                                 nn.Conv2d(in_channels, in_channels, 3),\n                                 nn.InstanceNorm2d(in_channels))\n\n    def forward(self, x):\n        return x + self.res(x)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T14:59:38.709416Z","iopub.execute_input":"2022-02-18T14:59:38.709678Z","iopub.status.idle":"2022-02-18T14:59:38.715768Z","shell.execute_reply.started":"2022-02-18T14:59:38.70964Z","shell.execute_reply":"2022-02-18T14:59:38.714764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Root directory for dataset\ndataroot = \"/kaggle/input/gan-getting-started\"\n# Number of workers for dataloader\nworkers = 2\n# Batch size during training\nbatch_size = 5\n# Spatial size of training images. All images will be resized to this\n# size using a transformer.\nimage_size = 256\n\nnum_epochs = 200\nlr_constant_epochs = 100\n# Learning rate for optimizers\nlr_d = 0.0002\nlr_g = 0.0002\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1\n# loss hyper parameter\nl = 10 # lambda","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:00:10.399665Z","iopub.execute_input":"2022-02-18T15:00:10.399958Z","iopub.status.idle":"2022-02-18T15:00:10.407891Z","shell.execute_reply.started":"2022-02-18T15:00:10.399927Z","shell.execute_reply":"2022-02-18T15:00:10.407104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MonetPhotoDataset(Dataset):\n    def __init__(self, data_dir, mode='train', transforms=None):\n        monet_dir = os.path.join(data_dir, 'monet_jpg')\n        photo_dir = os.path.join(data_dir, 'photo_jpg')\n        \n        if mode == 'train':\n            self.monet = [os.path.join(monet_dir, name) for name in sorted(os.listdir(monet_dir))[:295]]\n            self.photo = [os.path.join(photo_dir, name) for name in sorted(os.listdir(photo_dir))[:295]]\n        elif mode == 'test':\n            self.monet = [os.path.join(monet_dir, name) for name in sorted(os.listdir(monet_dir))[295:]]\n            self.photo = [os.path.join(photo_dir, name) for name in sorted(os.listdir(photo_dir))[295:301]]\n        \n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.monet)\n    \n    def __getitem__(self, index):\n        monet = self.monet[index]\n        photo = self.photo[index]\n        \n        monet = Image.open(monet)\n        photo = Image.open(photo)\n        \n        if self.transforms is not None:\n            monet = self.transforms(monet)\n            photo = self.transforms(photo)\n        \n        return monet, photo","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:00:40.166643Z","iopub.execute_input":"2022-02-18T15:00:40.166934Z","iopub.status.idle":"2022-02-18T15:00:40.176785Z","shell.execute_reply.started":"2022-02-18T15:00:40.166887Z","shell.execute_reply":"2022-02-18T15:00:40.175866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dset_trans = transforms.Compose([\n                transforms.Resize(image_size),\n                transforms.CenterCrop(image_size),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n            ])\n\ndataset = MonetPhotoDataset(dataroot,\"train\",dset_trans)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\ntest_dataset = MonetPhotoDataset(dataroot,\"test\",dset_trans)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n                                         shuffle=False, num_workers=workers)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:00:42.604151Z","iopub.execute_input":"2022-02-18T15:00:42.604399Z","iopub.status.idle":"2022-02-18T15:00:42.960219Z","shell.execute_reply.started":"2022-02-18T15:00:42.604372Z","shell.execute_reply":"2022-02-18T15:00:42.959463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport torch\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.functional import pad\nfrom torch.nn.modules import Module\nfrom torch.nn.modules.utils import _single, _pair, _triple\n\ndef conv2d_same_padding(input, weight, bias=None, stride=1, padding=1, dilation=1, groups=1, padding_mode = \"constant\"):\n    input_rows = input.size(2)\n    filter_rows = weight.size(2)\n    effective_filter_size_rows = (filter_rows - 1) * dilation[0] + 1\n    out_rows = (input_rows + stride[0] - 1) // stride[0]\n    padding_rows = max(0, (out_rows - 1) * stride[0] +\n                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n    rows_odd = (padding_rows % 2 != 0)\n    padding_cols = max(0, (out_rows - 1) * stride[0] +\n                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n    cols_odd = (padding_rows % 2 != 0)\n \n    if rows_odd or cols_odd:\n        input = pad(input, [0, int(cols_odd), 0, int(rows_odd)], mode=padding_mode)\n \n    return F.conv2d(input, weight, bias, stride,\n                  padding=(padding_rows // 2, padding_cols // 2),\n                  dilation=dilation, groups=groups)\n \nclass _ConvNd(Module):\n \n    def __init__(self, in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, transposed, output_padding, groups, bias):\n        super(_ConvNd, self).__init__()\n        if in_channels % groups != 0:\n            raise ValueError('in_channels must be divisible by groups')\n        if out_channels % groups != 0:\n            raise ValueError('out_channels must be divisible by groups')\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.transposed = transposed\n        self.output_padding = output_padding\n        self.groups = groups\n        if transposed:\n            self.weight = Parameter(torch.Tensor(\n                in_channels, out_channels // groups, *kernel_size))\n        else:\n            self.weight = Parameter(torch.Tensor(\n                out_channels, in_channels // groups, *kernel_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n \n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n \n    def __repr__(self):\n        s = ('{name}({in_channels}, {out_channels}, kernel_size={kernel_size}'\n             ', stride={stride}')\n        if self.padding != (0,) * len(self.padding):\n            s += ', padding={padding}'\n        if self.dilation != (1,) * len(self.dilation):\n            s += ', dilation={dilation}'\n        if self.output_padding != (0,) * len(self.output_padding):\n            s += ', output_padding={output_padding}'\n        if self.groups != 1:\n            s += ', groups={groups}'\n        if self.bias is None:\n            s += ', bias=False'\n        s += ')'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:03:08.449668Z","iopub.execute_input":"2022-02-18T15:03:08.449966Z","iopub.status.idle":"2022-02-18T15:03:08.469377Z","shell.execute_reply.started":"2022-02-18T15:03:08.449929Z","shell.execute_reply":"2022-02-18T15:03:08.468651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Conv2dWithSamePadding(_ConvNd): \n \n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True, padding_mode = \"zero\"):\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n        super(Conv2dWithSamePadding, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            False, _pair(0), groups, bias)\n        self.padding_mode = padding_mode\n        if self.padding_mode == \"zero\":\n            self.padding_mode == \"constant\"\n \n    def forward(self, x):\n        return conv2d_same_padding(x, self.weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups,self.padding_mode)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:03:10.703791Z","iopub.execute_input":"2022-02-18T15:03:10.704265Z","iopub.status.idle":"2022-02-18T15:03:10.711316Z","shell.execute_reply.started":"2022-02-18T15:03:10.704228Z","shell.execute_reply":"2022-02-18T15:03:10.710544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weights_init(m):\n    if isinstance(m, Conv2dWithSamePadding):\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    if isinstance(m, nn.Conv2d):\n        nn.init.normal_(m.weight.data, 0.0, 0.02)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:03:12.716942Z","iopub.execute_input":"2022-02-18T15:03:12.717461Z","iopub.status.idle":"2022-02-18T15:03:12.722232Z","shell.execute_reply.started":"2022-02-18T15:03:12.717426Z","shell.execute_reply":"2022-02-18T15:03:12.721265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the generator\n\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n\nnetG_Photo2Monet = Generator().to(device)\nnetG_Monet2Photo = Generator().to(device)\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.02.\nnetG_Photo2Monet.apply(weights_init)\nnetG_Monet2Photo.apply(weights_init)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:03:13.603882Z","iopub.execute_input":"2022-02-18T15:03:13.604578Z","iopub.status.idle":"2022-02-18T15:03:13.836624Z","shell.execute_reply.started":"2022-02-18T15:03:13.604538Z","shell.execute_reply":"2022-02-18T15:03:13.835873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"netD_Photo2Monet = Discriminator().to(device)\nnetD_Monet2Photo = Discriminator().to(device)\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.02.\nnetD_Photo2Monet.apply(weights_init)\nnetD_Monet2Photo.apply(weights_init)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:03:21.629159Z","iopub.execute_input":"2022-02-18T15:03:21.629678Z","iopub.status.idle":"2022-02-18T15:03:21.68521Z","shell.execute_reply.started":"2022-02-18T15:03:21.629641Z","shell.execute_reply":"2022-02-18T15:03:21.684377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer and other setup\n# Initialize BCELoss function\nfrom torch import optim\nfrom itertools import chain\ncriterion_GAN = nn.MSELoss()\n#criterion_GAN = nn.BCEWithLogitsLoss()\ncriterion_cycle = nn.L1Loss()\ncriterion_identity = nn.L1Loss()\n\n# Establish convention for real and fake labels during training\nreal_label = .9\nfake_label = 0.\n\n# Setup Adam optimizers for both G and D\noptimizerD_Photo2Monet = optim.Adam(netD_Photo2Monet.parameters(), lr=lr_d, betas=(beta1, 0.999))\noptimizerD_Monet2Photo = optim.Adam(netD_Monet2Photo.parameters(), lr=lr_d, betas=(beta1, 0.999))\noptimizerG = optim.Adam(chain(netG_Photo2Monet.parameters(), netG_Monet2Photo.parameters()), lr=lr_g, betas=(beta1, 0.999))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:03:25.292554Z","iopub.execute_input":"2022-02-18T15:03:25.292803Z","iopub.status.idle":"2022-02-18T15:03:25.301132Z","shell.execute_reply.started":"2022-02-18T15:03:25.292776Z","shell.execute_reply":"2022-02-18T15:03:25.300258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:03:28.051738Z","iopub.execute_input":"2022-02-18T15:03:28.052453Z","iopub.status.idle":"2022-02-18T15:03:28.058861Z","shell.execute_reply.started":"2022-02-18T15:03:28.052415Z","shell.execute_reply":"2022-02-18T15:03:28.057973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Loop\nimport torchvision.utils as vutils\nfrom tqdm import tqdm\nimport numpy as np\ntorch.autograd.set_detect_anomaly(True)\n# Lists to keep track of progress\nimg_list_monet = []\nimg_list_photo = []\nloss_d = []\nloss_g = []\nfrom tqdm import tqdm\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in tqdm(range(num_epochs)):\n    # For each batch in the dataloader\n    netD_Monet2Photo.train()\n    netD_Photo2Monet.train()\n    netG_Monet2Photo.train()\n    netG_Photo2Monet.train()\n    \n    for i, data in enumerate(dataloader, 0):\n        ############################\n        # objective function: L(Gx2y,Gy2x,Dy,Dx) = L1(Gx2y,Dy,x,y) + L2(Gy2x,Dx,x,y) + lambda1 * L3(G,F) + lambda2 * L4(G,F)\n        # L1, L2: the original loss of GANs, here using MSE loss\n        # L3: cycle loss, here using L1 loss\n        # L4: identity loss, here using L1 loss\n        # here, lambda2 = 0.5 * lambda1\n        ############################\n        data_monet = data[0].to(device)\n        data_photo = data[1].to(device)\n        b_size = data_monet.size(0)\n        label_real = torch.full((b_size,1,30,30), real_label, dtype=torch.float, device=device)\n        label_fake = torch.full((b_size,1,30,30), fake_label, dtype=torch.float, device=device)\n        \n        ############################\n        # (1) Update generators: \n        ###########################\n        netG_Monet2Photo.zero_grad()\n        netG_Photo2Monet.zero_grad()\n        \n        Monet_from_photo = netG_Photo2Monet(data_photo)\n        photo_from_monet = netG_Monet2Photo(data_monet)\n\n        # GAN loss\n        GAN_loss_Photo2Monet = criterion_GAN(netD_Photo2Monet(Monet_from_photo), label_real)\n        GAN_loss_Monet2Photo = criterion_GAN(netD_Monet2Photo(photo_from_monet), label_real)\n        GAN_loss = (GAN_loss_Photo2Monet + GAN_loss_Monet2Photo)\n\n        # cycle loss\n        monet2photo2monet = netG_Photo2Monet(photo_from_monet)\n        photo2monet2photo = netG_Monet2Photo(Monet_from_photo)\n        cycle_loss_Photo2Monet = criterion_cycle(photo2monet2photo, data_photo)\n        cycle_loss_Monet2Photo = criterion_cycle(monet2photo2monet, data_monet)\n        cycle_loss = (cycle_loss_Photo2Monet + cycle_loss_Monet2Photo)\n\n        # identity loss (need to check more)\n        monet_from_photo_by_net_p2m = netG_Photo2Monet(data_monet)\n        photo_from_monet_by_net_m2p = netG_Monet2Photo(data_photo)\n        identity_loss_p2m = criterion_identity(monet_from_photo_by_net_p2m, data_monet)\n        identity_loss_m2p = criterion_identity(photo_from_monet_by_net_m2p, data_photo)\n        identity_loss = (identity_loss_m2p + identity_loss_p2m)\n\n        # update paramenter\n        total_loss_G = GAN_loss + l * cycle_loss + 0.5 * l * identity_loss\n        total_loss_G.backward()\n        optimizerG.step()\n        \n        ############################\n        # (2) Update discriminators: \n        ###########################\n        \n        netD_Monet2Photo.zero_grad()\n        netD_Photo2Monet.zero_grad()\n        \n        total_d_loss = 0.\n        \n        # training the discriminator (Photo2Monet)\n        output_real = netD_Photo2Monet(data_monet)\n        output_fake = netD_Photo2Monet(Monet_from_photo.detach())\n        loss_real = criterion_GAN(output_real, label_real)\n        loss_fake = criterion_GAN(output_fake, label_fake)\n        loss_photo2Monet = (loss_real + loss_fake)\n        total_d_loss = total_d_loss + loss_photo2Monet.item()\n        loss_photo2Monet.backward()\n        optimizerD_Photo2Monet.step()\n\n        # traning the discriminator (Monet2Photo)\n        output_real = netD_Monet2Photo(data_photo)\n        output_fake = netD_Monet2Photo(photo_from_monet.detach())\n        loss_real = criterion_GAN(output_real, label_real)\n        loss_fake = criterion_GAN(output_fake, label_fake)\n        loss_Monet2Photo = (loss_real + loss_fake)\n        total_d_loss = total_d_loss + loss_Monet2Photo.item()\n        loss_Monet2Photo.backward()\n        optimizerD_Monet2Photo.step()\n        \n    loss_d.append(total_d_loss)\n    loss_g.append(total_loss_G.item())\n\n    if ((epoch+1) % 10) == 0:\n        print(f\"epoch {epoch + 1}\")\n        # print losses\n        print(f\"discriminator loss: phtot2monet: {loss_photo2Monet.item()}, monet2photo: {loss_Monet2Photo.item()}, total loss: {total_d_loss}\")\n        print(f\"generator loss: GAN: {GAN_loss.item()}, cycle: {cycle_loss.item()}, identity: {identity_loss.item()}\")\n        print(f\"generating images by generators in epoch {epoch+1}\")\n        # Checking both generators and visulization\n        netG_Monet2Photo.eval()\n        netG_Photo2Monet.eval()\n\n        ts_data = next(iter(test_dataloader))\n\n        ts_monet_data = ts_data[0].to(device)\n        ts_photo_data = ts_data[1].to(device)\n\n        monet = netG_Photo2Monet(ts_photo_data).detach()\n        photo = netG_Monet2Photo(ts_monet_data).detach()\n\n        nrows = ts_monet_data.size(0)\n\n        ts_photo_data = make_grid(ts_photo_data, nrow=nrows, normalize=True)\n        ts_monet_data = make_grid(ts_monet_data, nrow=nrows, normalize=True)\n        monet = make_grid(monet, nrow=nrows, normalize=True)\n        photo = make_grid(photo, nrow=nrows, normalize=True)\n\n        result = torch.cat((ts_photo_data, monet, ts_monet_data, photo), 1)\n        result = result.cpu().permute(1,2,0)\n\n        # show images \n        plt.axis(\"off\")\n        plt.figure(figsize=(1.5*nrows, 6))\n        plt.imshow(result)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:03:34.478485Z","iopub.execute_input":"2022-02-18T15:03:34.478864Z","iopub.status.idle":"2022-02-18T20:35:24.006856Z","shell.execute_reply.started":"2022-02-18T15:03:34.478824Z","shell.execute_reply":"2022-02-18T20:35:24.006079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(6,6))\nax = fig.add_axes([0.1, 0.1, 0.9, 0.9])\nx_epoch = np.arange(0, len(loss_d)).astype(dtype=np.str)\nax.plot(x_epoch, loss_d, 'r',label=\"Discriminator\")\nax.plot(x_epoch, loss_g, 'b',label=\"Generator\")\nax.set_xlabel(\"epoch\")\nax.set_ylabel(\"loss\")\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T20:35:33.472007Z","iopub.execute_input":"2022-02-18T20:35:33.472464Z","iopub.status.idle":"2022-02-18T20:35:34.854565Z","shell.execute_reply.started":"2022-02-18T20:35:33.472416Z","shell.execute_reply":"2022-02-18T20:35:34.853752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generating images and save them, waiting for metric\nphoto_dir = os.path.join(dataroot, 'photo_jpg')\nfiles = [os.path.join(photo_dir, name) for name in os.listdir(photo_dir)]\nlen(files)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T20:35:50.950501Z","iopub.execute_input":"2022-02-18T20:35:50.95098Z","iopub.status.idle":"2022-02-18T20:35:50.988214Z","shell.execute_reply.started":"2022-02-18T20:35:50.950929Z","shell.execute_reply":"2022-02-18T20:35:50.987499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dir = '/kaggle/images'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T20:36:44.196197Z","iopub.execute_input":"2022-02-18T20:36:44.196789Z","iopub.status.idle":"2022-02-18T20:36:44.203473Z","shell.execute_reply.started":"2022-02-18T20:36:44.196748Z","shell.execute_reply":"2022-02-18T20:36:44.202748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nto_image = transforms.ToPILImage()\n\nnetG_Photo2Monet.eval()\nfor i in range(0, len(files), batch_size):\n    # read images\n    imgs = []\n    for j in range(i, min(len(files), i+batch_size)):\n        img = Image.open(files[j])\n        img = generate_transforms(img)\n        imgs.append(img)\n    imgs = torch.stack(imgs, 0).to(device)\n    \n    # generate\n    fake_imgs = netG_Photo2Monet(imgs).detach().cpu()\n    \n    # save\n    for j in range(fake_imgs.size(0)):\n        img = fake_imgs[j].squeeze().permute(1, 2, 0)\n        img_arr = img.numpy()\n        img_arr = (img_arr - np.min(img_arr)) * 255 / (np.max(img_arr) - np.min(img_arr))\n        img_arr = img_arr.astype(np.uint8)\n        \n        img = to_image(img_arr)\n        _, name = os.path.split(files[i+j])\n        img.save(os.path.join(save_dir, name))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T20:36:56.845589Z","iopub.execute_input":"2022-02-18T20:36:56.845902Z","iopub.status.idle":"2022-02-18T20:40:30.014154Z","shell.execute_reply.started":"2022-02-18T20:36:56.845868Z","shell.execute_reply":"2022-02-18T20:40:30.013177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")\nprint(\"finished!\")","metadata":{"execution":{"iopub.status.busy":"2022-02-18T20:40:30.016045Z","iopub.execute_input":"2022-02-18T20:40:30.016315Z","iopub.status.idle":"2022-02-18T20:40:33.30984Z","shell.execute_reply.started":"2022-02-18T20:40:30.016279Z","shell.execute_reply":"2022-02-18T20:40:33.309089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}